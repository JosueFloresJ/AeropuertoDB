{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79885a4b",
   "metadata": {},
   "source": [
    "# Web Scraping para la Recopilación de Datos de Aeropuertos en América\n",
    "\n",
    "## Introducción:\n",
    "\n",
    "Este Jupyter Notebook tiene como objetivo principal la recopilación de datos de aeropuertos en América mediante técnicas de web scraping. En el contexto de nuestro proyecto de base de datos orientada a grafos para la gestión de aeropuertos, es esencial contar con información actualizada y precisa sobre los aeropuertos en la región.\n",
    "\n",
    "El web scraping nos permitirá extraer datos relevantes de fuentes en línea, como sitios web de autoridades de aviación y aerolíneas. Estos datos incluirán detalles sobre ubicaciones geográficas, nombres de aeropuertos, servicios disponibles, rutas operadas y otros atributos esenciales.\n",
    "\n",
    "## Objetivos:\n",
    "\n",
    "Obtener datos de aeropuertos en América a partir de fuentes en línea.\n",
    "Extraer información clave, como nombres de aeropuertos, ubicaciones y detalles operativos.\n",
    "Almacenar los datos recopilados en un formato adecuado para su posterior análisis y uso en nuestra base de datos orientada a grafos.\n",
    "\n",
    "### Autor\n",
    "\n",
    "Este Python Notebook está hecho por **Manrique Camacho P. (C01554)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364df272",
   "metadata": {},
   "source": [
    "## Importación de librerias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ab0a4",
   "metadata": {},
   "source": [
    "* **Requests**: La biblioteca requests le permite al usuario realizar solicitudes HTTP a las páginas web que desee analizar, facilitando la descarga del contenido HTML de dichas páginas para su posterior procesamiento.\n",
    "\n",
    "* **Beautiful Soup (bs4)**: Beautiful Soup es una herramienta útil para analizar y buscar elementos HTML en el contenido descargado. Permite al usuario buscar y extraer información específica de las páginas web, como títulos, párrafos, enlaces y más.\n",
    "\n",
    "* **Selenium**: Cuando los sitios web utilizan JavaScript para cargar contenido dinámico, selenium se convierte en una opción valiosa. Con esta biblioteca, el usuario puede automatizar un navegador web para interactuar con el sitio web y extraer datos de páginas que requieren interacción.\n",
    "\n",
    "* **Pandas**: Pandas es una biblioteca esencial para estructurar y manipular los datos extraídos. Permite al usuario crear DataFrames para organizar los datos en filas y columnas, lo que facilita las operaciones de limpieza, filtrado y procesamiento.\n",
    "\n",
    "* **Matplotlib y Seaborn**: Estas bibliotecas son útiles para crear gráficos y visualizaciones si se desea presentar los datos recopilados de manera visual. Permiten personalizar gráficos para mostrar tendencias o patrones en los datos.\n",
    "\n",
    "* **GeoPandas**: Si el proyecto involucra datos geoespaciales o la representación geográfica de aeropuertos, GeoPandas es una herramienta útil. Permite al usuario trabajar con datos geoespaciales, crear mapas y realizar análisis geoespaciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2016cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import ui\n",
    "import chromedriver_autoinstaller\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a18a75",
   "metadata": {},
   "source": [
    "## Web Scraping para Recopilación de Datos\n",
    "\n",
    "En esta sección, se explorará el web scraping, una técnica automatizada para obtener datos de sitios web. Los temas clave incluyen:\n",
    "\n",
    "* **Solicitudes HTTP**: Se utilizará la biblioteca requests para descargar el contenido HTML de las páginas web.\n",
    "* **Análisis HTML**: Se empleará Selenium para interactuar con sitios web dinámicos y extraer información relevante.\n",
    "* **Recopilación de Datos**: Se llevará a cabo la recolección de datos desde los sitios web seleccionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2ca86baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_autoinstaller.install()  # Instala automáticamente la versión correcta de ChromeDriver\n",
    "ae_centralamerica = ['pty/airport-panama-city-tocumen','sjo/airport-san-jose-juan-santamaria',\n",
    "                    'sal/airport-san-salvador-cuscatlan','mga/airport-managua',\n",
    "                    'tgu/airport-tegucigalpa'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edfb3174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "res = requests.get(\"https://airportinfo.live/arrivals/sjo/airport-san-jose-juan-santamaria\",headers ={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "60ed4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def airport_basic(extension): #Nos dara la información general del aeropuerto\n",
    "\n",
    "    res = requests.get(f\"https://airportinfo.live/arrivals/{extension}\",headers ={'User-Agent': 'Mozilla/5.0'})\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    nombre = soup.find_all('strong')[-1].get_text(strip=True)\n",
    "    telephone = ' '.join(soup.find_all('li')[-3].text.split()[2:])\n",
    "    timezone = ' '.join(soup.find_all('li')[-2].text.split()[5:])\n",
    "\n",
    "    data = []\n",
    "    for i in soup.find_all('li')[-4]:\n",
    "        data.append(i)\n",
    "    \n",
    "    location = data[2]\n",
    "    \n",
    "    info_ae = [nombre, location, timezone, telephone]\n",
    "    \n",
    "    return info_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6ff1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://airportinfo.live/arrivals/sjo/airport-san-jose-juan-santamaria\")\n",
    "hora = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"timeStart\"]')))\n",
    "driver.execute_script(\"return arguments[0].scrollIntoView(true);\", hora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1bbcf672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Salvador [SAL]\n"
     ]
    }
   ],
   "source": [
    "pais = driver.find_element(By.XPATH, '//*[@id=\"AV627\"]/td[2]/a')\n",
    "print(pais.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d2382624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', '627', 'San', 'Salvador', '[SAL]', 'Avianca', '21:50', '[Oct-8]', '23:05', '23:00', 'TM', 'ARRIVED', 'AC', '6137', 'San', 'Salvador', '[SAL]', 'Air', 'Canada', '21:50', '[Oct-8]', '23:05', '23:00', 'TM', 'ARRIVED', 'Codeshare', 'Flight', '-', 'operated', 'by', 'AV627', 'IB', '7803', 'San', 'Salvador', '[SAL]', 'Iberia', '21:50', '[Oct-8]', '23:05', '23:00', 'TM', 'ARRIVED', 'Codeshare', 'Flight', '-', 'operated', 'by', 'AV627', 'AV', '651', 'Guatemala', 'City', '[GUA]', 'Avianca', '21:30', '[Oct-8]', '23:05', '22:56', 'TM', 'ARRIVED', 'AC', '6122', 'Guatemala', 'City', '[GUA]', '21:30', '[Oct-8]', '23:05', '22:56', 'TM', 'ARRIVED', 'Codeshare', 'Flight', '-', 'operated', 'by', 'AV651', 'AS', '1390', 'Los', 'Angeles', '[LAX]', 'Alaska', 'Airlines', '16:53', '17:40', '[Oct-8]', 'T664A', '23:44', '23:58', 'TMHG', 'IN', 'AIR']\n"
     ]
    }
   ],
   "source": [
    "body = driver.find_element(By.XPATH, '//*[@id=\"San José-arrivals\"]/tbody')\n",
    "print(body.text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc248e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
